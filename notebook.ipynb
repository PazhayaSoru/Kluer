{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36ce563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY= os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "SERPER_API_KEY=os.getenv(\"SERPER_API_KEY\")\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "NEO4J_URI=\"neo4j+s://4a1ee668.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"rHf5fPd0fExs1hJM5cSexQF3Qo7rLNsP0rm3daQYh1o\"\n",
    "AURA_INSTANCEID=\"4a1ee668\"\n",
    "AURA_INSTANCENAME=\"Free instance\"\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY \n",
    "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] =  LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
    "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
    "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e552e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdc884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thede\\AppData\\Local\\Temp\\ipykernel_22484\\1759000609.py:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph=Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "graph=Neo4jGraph(\n",
    "  url=NEO4J_URI,\n",
    "  username=NEO4J_USERNAME,\n",
    "  password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c454766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x1b4c554d8b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804eb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01281d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nFirst, I recognize that the user is asking for the sum of 6 and 2.\\n\\nTo solve this, I need to add the two numbers together.\\n\\nAdding 6 and 2 results in 8.\\n\\nTherefore, the answer to 6 plus 2 is 8.\\n</think>\\n\\n**Solution:**\\n\\nTo find the sum of 6 and 2, follow these simple steps:\\n\\n1. **Start with the first number:**\\n   \\\\[\\n   6\\n   \\\\]\\n\\n2. **Add the second number to the first number:**\\n   \\\\[\\n   6 + 2\\n   \\\\]\\n\\n3. **Calculate the total:**\\n   \\\\[\\n   6 + 2 = 8\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{8}\\n\\\\]', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 9, 'total_tokens': 175, 'completion_time': 0.603636364, 'prompt_time': 0.000334845, 'queue_time': 0.061656794, 'total_time': 0.603971209}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-2292b37c-3c91-4ec4-a97d-9697b76b533d-0', usage_metadata={'input_tokens': 9, 'output_tokens': 166, 'total_tokens': 175})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is 6+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75044774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    "At 6 45 pm, on March 13th 2025, i had two vada pavs at shree mithai\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f94e0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nAt 6 45 pm, on March 13th 2025, i had two vada pavs at shree mithai\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [Document(page_content=text)]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7049ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478d80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents=llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680d3d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphDocument(nodes=[Node(id='I', type='Person', properties={}), Node(id='Vada Pav', type='Food', properties={}), Node(id='Shree Mithai', type='Place', properties={}), Node(id='6:45 Pm', type='Time', properties={}), Node(id='March 13Th, 2025', type='Date', properties={})], relationships=[Relationship(source=Node(id='I', type='Person', properties={}), target=Node(id='Vada Pav', type='Food', properties={}), type='HAD', properties={}), Relationship(source=Node(id='I', type='Person', properties={}), target=Node(id='6:45 Pm', type='Time', properties={}), type='AT', properties={}), Relationship(source=Node(id='I', type='Person', properties={}), target=Node(id='March 13Th, 2025', type='Date', properties={}), type='AT', properties={}), Relationship(source=Node(id='I', type='Person', properties={}), target=Node(id='Shree Mithai', type='Place', properties={}), type='AT', properties={})], source=Document(metadata={}, page_content='\\nAt 6 45 pm, on March 13th 2025, i had two vada pavs at shree mithai\\n'))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560f0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    " graph_documents,\n",
    " baseEntityLabel=True,\n",
    " include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a954c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thede\\OneDrive\\Desktop\\langraph-end-to-end\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb6c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from typing import Tuple,List,Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f68e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=['text'],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "073754cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6949c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thede\\OneDrive\\Desktop\\langraph-end-to-end\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3549: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Entities(BaseModel):\n",
    "  \"\"\" Identifying information from \"\"\"\n",
    "\n",
    "  names: List[str] = Field(\n",
    "    ...,\n",
    "    description=\"all the person, organization or business entities that appear in the text\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4520f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d64380",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"You are extracting relevant entities like organization, person, food etc from the text\",\n",
    "    ),\n",
    "    (\"system\",\n",
    "     \"Use the given format to extract information from the following\"\n",
    "     \"input: {question}\"),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c40fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bdc1f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entities(names=['shree mithai'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_chain.invoke({\"question\":\"what did i have at shree mithai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e669105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dceba399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\"{word}~2 AND\" \n",
    "    full_text_query += f\"{words[-1]}~2\"\n",
    "    return full_text_query.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a746f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_retriever(question: str) -> str:\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node, score\n",
    "            CALL {\n",
    "                WITH node\n",
    "                MATCH (node)-[r:MENTIONS]->(neighbor)\n",
    "                RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "                UNION ALL\n",
    "                WITH node\n",
    "                MATCH (node)<-[r:MENTIONS]-(neighbor)\n",
    "                RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cad25c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thede\\AppData\\Local\\Temp\\ipykernel_22484\\4044109123.py:3: LangChainDeprecationWarning: The function `remove_lucene_chars` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the function exists in the :meth:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :meth:`~langchain-neo4j` and import as `from :meth:`~langchain_neo4j.vectorstores.neo4j_vector import remove_lucene_chars``.\n",
      "  words = [el for el in remove_lucene_chars(input).split() if el]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 4, column: 13, offset: 118} for query: \"\\n            CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node, score\\n            CALL {\\n                WITH node\\n                MATCH (node)-[r:MENTIONS]->(neighbor)\\n                RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n                UNION ALL\\n                WITH node\\n                MATCH (node)<-[r:MENTIONS]-(neighbor)\\n                RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37e97ecf516d1dd7eec6d70d1a822ee1 - MENTIONS -> Shree Mithai\n"
     ]
    }
   ],
   "source": [
    "print(structured_retriever(\"what did i eat at shree mithai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d0447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question : str):\n",
    "  print(f\"Search query: {question}\")\n",
    "  structured_data = structured_retriever(question)\n",
    "  unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "  final_data = f\"\"\"\n",
    "    Structured Data:\n",
    "    {structured_data}\n",
    "Unstructured data:\n",
    "{\"Document \".join(unstructured_data)}\n",
    "\"\"\"\n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e212850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question\n",
    "to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a19a96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99ea1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff770b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_history(chat_history : List[Tuple[str,str]]) -> List:\n",
    "  buffer = []\n",
    "  for human, ai in chat_history:\n",
    "    buffer.append(HumanMessage(content=human))\n",
    "    buffer.append(AIMessage(content=ai))\n",
    "  return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46e4d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch,RunnableLambda,RunnablePassthrough,RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "611b6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4b2c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    \n",
    "    (\n",
    "      RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "        run_name=\"HasChatHistoryCheck\"\n",
    "    ),\n",
    "        # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: format_chat_history(x[\"chat_history\"])\n",
    "        ) |\n",
    "        CONDENSE_QUESTION_PROMPT |\n",
    "        ChatGroq(\n",
    "          model=\"deepseek-r1-distill-llama-70b\",\n",
    "          temperature=0\n",
    "        ) |\n",
    "        StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x: x[\"question\"]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fca51676",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fd6b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7967d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "  RunnableParallel(\n",
    "    {\n",
    "      \"context\":_search_query | retriever,\n",
    "      \"question\":RunnablePassthrough(),\n",
    "    }\n",
    "  )\n",
    "  | prompt\n",
    "  |llm\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45aaed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: How many vada pav did i eat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 4, column: 13, offset: 118} for query: \"\\n            CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node, score\\n            CALL {\\n                WITH node\\n                MATCH (node)-[r:MENTIONS]->(neighbor)\\n                RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n                UNION ALL\\n                WITH node\\n                MATCH (node)<-[r:MENTIONS]-(neighbor)\\n                RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out how many vada pav the person ate. Let me look at the information provided. \\n\\nIn the structured data, there\\'s a mention of \"Vada Pav,\" which is the dish in question. Then, in the unstructured text, it says, \"At 6 45 pm, on March 13th 2025, i had two vada pavs at shree mithai.\" \\n\\nSo, the key part here is \"had two vada pavs.\" That clearly states the number. I don\\'t think there\\'s any ambiguity here. The person explicitly mentions having two of them. \\n\\nI should make sure I\\'m only using the provided context and not adding any outside information. The question is straightforward, asking for the count, so the answer should be concise and based solely on the given text.\\n\\nI don\\'t see any other numbers or mentions of vada pav elsewhere, so two is the correct answer. No need to overcomplicate it.\\n</think>\\n\\nYou ate two vada pavs.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"How many vada pav did i eat\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61075d",
   "metadata": {},
   "source": [
    "OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thede\\OneDrive\\Desktop\\langraph-end-to-end\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "best\n",
      "preparation\n",
      "for\n",
      "tomorrow\n",
      "is\n",
      "doing your\n",
      "best today -\n"
     ]
    }
   ],
   "source": [
    "import easyocr as eocr\n",
    "reader = eocr.Reader(['en'])\n",
    "result = reader.readtext(\"istockphoto-1758363728-612x612.jpg\")\n",
    "\n",
    "for (bbox,text,prob) in result:\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b38969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
